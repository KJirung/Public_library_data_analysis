{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# 소수를 e로 표현하지 않도록 하기\n",
    "pd.options.display.float_format = \"{:.5f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_2007 = pd.read_csv(\"../final_data/lib_2007.csv\", index_col=0)\n",
    "lib_2008 = pd.read_csv(\"../final_data/lib_2008.csv\", index_col=0)\n",
    "lib_2009 = pd.read_csv(\"../final_data/lib_2009.csv\", index_col=0)\n",
    "lib_2010 = pd.read_csv(\"../final_data/lib_2010.csv\", index_col=0)\n",
    "lib_2011 = pd.read_csv(\"../final_data/lib_2011.csv\", index_col=0)\n",
    "lib_2012 = pd.read_csv(\"../final_data/lib_2012.csv\", index_col=0)\n",
    "lib_2013 = pd.read_csv(\"../final_data/lib_2013.csv\", index_col=0)\n",
    "lib_2014 = pd.read_csv(\"../final_data/lib_2014.csv\", index_col=0)\n",
    "lib_2015 = pd.read_csv(\"../final_data/lib_2015.csv\", index_col=0)\n",
    "lib_2016 = pd.read_csv(\"../final_data/lib_2016.csv\", index_col=0)\n",
    "lib_2017 = pd.read_csv(\"../final_data/lib_2017.csv\", index_col=0)\n",
    "lib_2018 = pd.read_csv(\"../final_data/lib_2018.csv\", index_col=0)\n",
    "lib_2019 = pd.read_csv(\"../final_data/lib_2019.csv\", index_col=0)\n",
    "lib_2020 = pd.read_csv(\"../final_data/lib_2020.csv\", index_col=0)\n",
    "lib_2021 = pd.read_csv(\"../final_data/lib_2021.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lib = pd.concat([lib_2007,lib_2008,lib_2009,lib_2010,lib_2011,lib_2012,lib_2013,lib_2014,lib_2015, lib_2016, lib_2017])\n",
    "valid_lib = pd.concat([lib_2018, lib_2019])\n",
    "test_lib = pd.concat([lib_2020,lib_2021])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set의 데이터 수 : 9026\n",
      "valid set의 데이터 수 : 2222\n",
      "test set의 데이터 수 : 2370\n"
     ]
    }
   ],
   "source": [
    "# train, test set의 데이터 수 구하기\n",
    "print(f\"train set의 데이터 수 : {len(train_lib)}\")\n",
    "print(f\"valid set의 데이터 수 : {len(valid_lib)}\")\n",
    "print(f\"test set의 데이터 수 : {len(test_lib)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이상치 제거 후 train set의 데이터 수 : 8184\n",
      "이상치 제거 후 valid set의 데이터 수 : 2178\n",
      "이상치 제거 후 test set의 데이터 수 : 2310\n"
     ]
    }
   ],
   "source": [
    "# 사분위수를 활용하여 비율 충족도의 이상치 행 제거하기\n",
    "def del_outlier(df, col):\n",
    "    q1 = df[col].quantile(0.10)\n",
    "    q3 = df[col].quantile(0.90)\n",
    "    iqr = q3 - q1 \n",
    "    boundary = 1.5 * iqr \n",
    "\n",
    "    upper_index = df[df[col] > q3 + boundary].index\n",
    "    lower_index = df[df[col] < q1 - boundary].index \n",
    "\n",
    "    df.drop(upper_index, inplace = True)\n",
    "    df.drop(lower_index, inplace = True)\n",
    "\n",
    "    return df\n",
    "\n",
    "del_outlier(train_lib, \"future_acq_budget_settlement\")\n",
    "del_outlier(valid_lib, \"future_acq_budget_settlement\")\n",
    "del_outlier(test_lib, \"future_acq_budget_settlement\") \n",
    "\n",
    "# 이상치 제거 후 train, test set의 데이터 수 구하기\n",
    "print(f\"이상치 제거 후 train set의 데이터 수 : {len(train_lib)}\")\n",
    "print(f\"이상치 제거 후 valid set의 데이터 수 : {len(valid_lib)}\")\n",
    "print(f\"이상치 제거 후 test set의 데이터 수 : {len(test_lib)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Min-Max Scaler 생성\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 모든 변수에 대해 Min-Max Scaling 적용\n",
    "# 스케일러를 학습 데이터에 대해 fit\n",
    "scaler.fit(train_lib.drop(columns=[\"future_acq_budget_settlement\"]))\n",
    "\n",
    "# 학습 데이터에 대해 변환\n",
    "train_lib_scaled = pd.DataFrame(scaler.transform(train_lib.drop(columns=[\"future_acq_budget_settlement\"])), \n",
    "                                columns=train_lib.columns[:-1])\n",
    "\n",
    "# 타겟 변수에 대해 스케일링 적용\n",
    "scaler_y = MinMaxScaler()\n",
    "train_y_scaled = scaler_y.fit_transform(train_lib[['future_acq_budget_settlement']])\n",
    "\n",
    "# 학습 데이터에 스케일된 타겟 변수 추가\n",
    "train_lib_scaled['future_acq_budget_settlement_scaled'] = train_y_scaled\n",
    "\n",
    "# 검증 데이터에 대해 변환\n",
    "valid_lib_scaled = pd.DataFrame(scaler.transform(valid_lib.drop(columns=[\"future_acq_budget_settlement\"])), \n",
    "                                columns=train_lib.columns[:-1])\n",
    "valid_y_scaled = scaler_y.transform(valid_lib[['future_acq_budget_settlement']])\n",
    "valid_lib_scaled['future_acq_budget_settlement_scaled'] = valid_y_scaled\n",
    "\n",
    "# 테스트 데이터에 대해 변환\n",
    "test_lib_scaled = pd.DataFrame(scaler.transform(test_lib.drop(columns=[\"future_acq_budget_settlement\"])), \n",
    "                               columns=train_lib.columns[:-1])\n",
    "test_y_scaled = scaler_y.transform(test_lib[['future_acq_budget_settlement']])\n",
    "test_lib_scaled['future_acq_budget_settlement_scaled'] = test_y_scaled\n",
    "\n",
    "# X_train, Y_train 구성하기 (스케일된 타겟 사용)\n",
    "X_train = train_lib_scaled.drop(columns=[\"future_acq_budget_settlement_scaled\"])\n",
    "Y_train = train_lib_scaled[\"future_acq_budget_settlement_scaled\"]\n",
    "\n",
    "# X_valid, Y_valid 구성하기 (스케일된 타겟 사용)\n",
    "X_valid = valid_lib_scaled.drop(columns=[\"future_acq_budget_settlement_scaled\"])\n",
    "Y_valid = valid_lib_scaled[\"future_acq_budget_settlement_scaled\"]\n",
    "\n",
    "# X_test, Y_test 구성하기 (스케일된 타겟 사용)\n",
    "X_test = test_lib_scaled.drop(columns=[\"future_acq_budget_settlement_scaled\"])\n",
    "Y_test = test_lib_scaled[\"future_acq_budget_settlement_scaled\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"prog_sessions\",\t\"local_mat\", \"prog_partic\", \"service_recip\", \"self_srv_machines\", \"interlib_loans\", \n",
    "                   \"reg_members\", \"borrowers\", \"user_ed_partic\", \"web_access\", \"open_days\", \n",
    "                   \"avg_week_hours\", \"disab_mat\", \"ann_weeding\", \"support_staff\", \"info_serv_requests\"]\n",
    "\n",
    "X_train.drop(columns=columns_to_drop, inplace=True)\n",
    "X_valid.drop(columns=columns_to_drop, inplace=True)\n",
    "X_test.drop(columns=columns_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 14:13:28.857068: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-15 14:13:29.031418: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-15 14:13:29.031477: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-15 14:13:29.031490: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-15 14:13:29.074809: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-15 14:13:38.546653: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 31141 MB memory:  -> device: 0, name: Tesla V100-SXM2-32GB, pci bus id: 0000:1b:00.0, compute capability: 7.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(X_train.shape[1], 1), return_sequences=True)) \n",
    "model.add(Dropout(0.2)) \n",
    "model.add(LSTM(100)) \n",
    "model.add(Dropout(0.2))\n",
    "  \n",
    "model.add(Dense(1, activation='linear'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-15 14:13:51.088519: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8907\n",
      "2024-09-15 14:13:54.401592: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f6998211e10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-09-15 14:13:54.401665: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-SXM2-32GB, Compute Capability 7.0\n",
      "2024-09-15 14:13:54.412343: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-09-15 14:13:54.836520: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 18s 19ms/step - loss: 0.0123 - mse: 0.0123 - val_loss: 0.0151 - val_mse: 0.0151\n",
      "Epoch 2/50\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.0109 - mse: 0.0109 - val_loss: 0.0143 - val_mse: 0.0143\n",
      "Epoch 3/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0105 - mse: 0.0105 - val_loss: 0.0140 - val_mse: 0.0140\n",
      "Epoch 4/50\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.0101 - mse: 0.0101 - val_loss: 0.0149 - val_mse: 0.0149\n",
      "Epoch 5/50\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.0100 - mse: 0.0100 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 6/50\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.0098 - mse: 0.0098 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 7/50\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 0.0096 - mse: 0.0096 - val_loss: 0.0139 - val_mse: 0.0139\n",
      "Epoch 8/50\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.0095 - mse: 0.0095 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 9/50\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.0092 - mse: 0.0092 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 10/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0091 - mse: 0.0091 - val_loss: 0.0142 - val_mse: 0.0142\n",
      "Epoch 11/50\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.0090 - mse: 0.0090 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 12/50\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.0088 - mse: 0.0088 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 13/50\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.0087 - mse: 0.0087 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 14/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0084 - mse: 0.0084 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 15/50\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.0086 - mse: 0.0086 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 16/50\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 17/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0083 - mse: 0.0083 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 18/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0082 - mse: 0.0082 - val_loss: 0.0130 - val_mse: 0.0130\n",
      "Epoch 19/50\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 0.0081 - mse: 0.0081 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 20/50\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 21/50\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 22/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0079 - mse: 0.0079 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 23/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0134 - val_mse: 0.0134\n",
      "Epoch 24/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0133 - val_mse: 0.0133\n",
      "Epoch 25/50\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.0077 - mse: 0.0077 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 26/50\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0146 - val_mse: 0.0146\n",
      "Epoch 27/50\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 28/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0076 - mse: 0.0076 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 29/50\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0135 - val_mse: 0.0135\n",
      "Epoch 30/50\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.0074 - mse: 0.0074 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 31/50\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 32/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 33/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0075 - mse: 0.0075 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 34/50\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 35/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 36/50\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0131 - val_mse: 0.0131\n",
      "Epoch 37/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 38/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 39/50\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 40/50\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 41/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0072 - mse: 0.0072 - val_loss: 0.0128 - val_mse: 0.0128\n",
      "Epoch 42/50\n",
      "256/256 [==============================] - 3s 10ms/step - loss: 0.0073 - mse: 0.0073 - val_loss: 0.0132 - val_mse: 0.0132\n",
      "Epoch 43/50\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 44/50\n",
      "256/256 [==============================] - 3s 13ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 45/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0129 - val_mse: 0.0129\n",
      "Epoch 46/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 47/50\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.0071 - mse: 0.0071 - val_loss: 0.0126 - val_mse: 0.0126\n",
      "Epoch 48/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0124 - val_mse: 0.0124\n",
      "Epoch 49/50\n",
      "256/256 [==============================] - 3s 12ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0127 - val_mse: 0.0127\n",
      "Epoch 50/50\n",
      "256/256 [==============================] - 3s 11ms/step - loss: 0.0070 - mse: 0.0070 - val_loss: 0.0128 - val_mse: 0.0128\n"
     ]
    }
   ],
   "source": [
    "# 모델 컴파일\n",
    "optimizer = Adam(learning_rate=0.001)  # Adam 옵티마이저와 학습률 설정\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error', metrics='mse')  # 회귀 문제의 경우 MSE 손실 함수 사용\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(X_train, Y_train, epochs=50, batch_size=32, validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 3s 5ms/step\n",
      "MSE: 0.0066236406564712524\n",
      "RMSE: 0.08138575404882431\n",
      "MAE: 0.050687018781900406\n",
      "SMAPE: 33.751403079396304\n",
      "R²: 0.7459173580412783\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "# y_pred와 Y_test를 1차원으로 변환\n",
    "y_pred = y_pred.flatten()\n",
    "Y_train = Y_train.to_numpy().flatten()\n",
    "\n",
    "# MSE\n",
    "mse = MeanSquaredError()\n",
    "mse.update_state(Y_train, y_pred)\n",
    "mse_result = mse.result().numpy()\n",
    "\n",
    "# RMSE\n",
    "rmse_result = np.sqrt(mse_result)\n",
    "\n",
    "# MAE\n",
    "mae = MeanAbsoluteError()\n",
    "mae.update_state(Y_train, y_pred)\n",
    "mae_result = mae.result().numpy()\n",
    "\n",
    "# SMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n",
    "\n",
    "smape_result = smape(Y_train, y_pred)\n",
    "\n",
    "# R² (R Squared)\n",
    "r2_result = r2_score(Y_train, y_pred)\n",
    "\n",
    "print(f'MSE: {mse_result}')\n",
    "print(f'RMSE: {rmse_result}')\n",
    "print(f'MAE: {mae_result}')\n",
    "print(f'SMAPE: {smape_result}')\n",
    "print(f'R²: {r2_result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69/69 [==============================] - 1s 6ms/step\n",
      "MSE: 0.012783690355718136\n",
      "RMSE: 0.11306498199701309\n",
      "MAE: 0.06760044395923615\n",
      "SMAPE: 37.611163811906614\n",
      "R²: 0.5358405362290457\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(X_valid)\n",
    "\n",
    "# y_pred와 Y_valid를 1차원으로 변환\n",
    "y_pred = y_pred.flatten()\n",
    "Y_valid = Y_valid.to_numpy().flatten()\n",
    "\n",
    "# MSE\n",
    "mse = MeanSquaredError()\n",
    "mse.update_state(Y_valid, y_pred)\n",
    "mse_result = mse.result().numpy()\n",
    "\n",
    "# RMSE\n",
    "rmse_result = np.sqrt(mse_result)\n",
    "\n",
    "# MAE\n",
    "mae = MeanAbsoluteError()\n",
    "mae.update_state(Y_valid, y_pred)\n",
    "mae_result = mae.result().numpy()\n",
    "\n",
    "# SMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n",
    "\n",
    "smape_result = smape(Y_valid, y_pred)\n",
    "\n",
    "# R² (R Squared)\n",
    "r2_result = r2_score(Y_valid, y_pred)\n",
    "\n",
    "print(f'MSE: {mse_result}')\n",
    "print(f'RMSE: {rmse_result}')\n",
    "print(f'MAE: {mae_result}')\n",
    "print(f'SMAPE: {smape_result}')\n",
    "print(f'R²: {r2_result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73/73 [==============================] - 0s 5ms/step\n",
      "MSE: 0.008723324164748192\n",
      "RMSE: 0.09339873492717743\n",
      "MAE: 0.059668801724910736\n",
      "SMAPE: 36.050837006058636\n",
      "R²: 0.6721522495062036\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# 예측\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# y_pred와 Y_test를 1차원으로 변환\n",
    "y_pred = y_pred.flatten()\n",
    "Y_test = Y_test.to_numpy().flatten()\n",
    "\n",
    "# MSE\n",
    "mse = MeanSquaredError()\n",
    "mse.update_state(Y_test, y_pred)\n",
    "mse_result = mse.result().numpy()\n",
    "\n",
    "# RMSE\n",
    "rmse_result = np.sqrt(mse_result)\n",
    "\n",
    "# MAE\n",
    "mae = MeanAbsoluteError()\n",
    "mae.update_state(Y_test, y_pred)\n",
    "mae_result = mae.result().numpy()\n",
    "\n",
    "# SMAPE (Symmetric Mean Absolute Percentage Error)\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n",
    "\n",
    "smape_result = smape(Y_test, y_pred)\n",
    "\n",
    "# R² (R Squared)\n",
    "r2_result = r2_score(Y_test, y_pred)\n",
    "\n",
    "print(f'MSE: {mse_result}')\n",
    "print(f'RMSE: {rmse_result}')\n",
    "print(f'MAE: {mae_result}')\n",
    "print(f'SMAPE: {smape_result}')\n",
    "print(f'R²: {r2_result}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/jw_tensorflow/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/root/anaconda3/envs/jw_tensorflow/lib/python3.11/site-packages/shap/explainers/_deep/deep_tf.py:99: UserWarning: Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\n",
      "  warnings.warn(\"Your TensorFlow version is newer than 2.4.0 and so graph support has been removed in eager mode and some static graphs may not be supported. See PR #1483 for discussion.\")\n",
      "/root/anaconda3/envs/jw_tensorflow/lib/python3.11/site-packages/shap/explainers/_deep/deep_tf.py:155: UserWarning: You have provided over 5k background samples! For better performance consider using smaller random sample.\n",
      "  warnings.warn(\"You have provided over 5k background samples! For better performance consider using smaller random sample.\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'sequential' (type Sequential).\n\nInput 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (8184, 20)\n\nCall arguments received by layer 'sequential' (type Sequential):\n  • inputs=['      dom_books  for_books  nonbook_mat  elec_mat  serials  ann_growth  \\\\\\n0       0.00000    0.00000      0.00000   0.00000  0.00431     0.00033   \\n1       0.08417    0.00000      0.01129   0.00026  0.00280     0.00142   \\n2       0.04455    0.00000      0.00452   0.00000  0.00374     0.00670   \\n3       0.33166    0.00000      0.01953   0.00107  0.01013     0.01156   \\n4       0.11052    0.00000      0.02472   0.00026  0.00676     0.00309   \\n...         ...        ...          ...       ...      ...         ...   \\n8179    0.05988    0.03277      0.00641   0.00039  0.00359     0.00306   \\n8180    0.04843    0.00229      0.00786   0.00049  0.00036     0.00171   \\n8181    0.02278    0.00318      0.00471   0.00255  0.00122     0.00192   \\n8182    0.00745    0.01823      0.00000   0.00000  0.02817     0.00066   \\n8183    0.04277    0.01639      0.00998   0.00000  0.00539     0.00090   \\n\\n      lib_site_area  lib_build_area  total_seats  user_comps  full_time  \\\\\\n0           0.00610         0.00725      0.09126     0.00000    0.00000   \\n1           0.00516         0.01455      0.19263     0.09023    0.15714   \\n2           0.00284         0.00496      0.04816     0.06391    0.07143   \\n3           0.00291         0.00314      0.04816     0.05639    0.12857   \\n4           0.00157         0.01015      0.19263     0.11278    0.21429   \\n...             ...             ...          ...         ...        ...   \\n8179        0.00507         0.01481      0.05851     0.08647    0.07143   \\n8180        0.00227         0.00531      0.05177     0.12030    0.10000   \\n8181        0.00027         0.00115      0.02071     0.00376    0.05714   \\n8182        0.00154         0.00062      0.01156     0.00000    0.05714   \\n8183        0.00195         0.00156      0.03299     0.00752    0.07143   \\n\\n      part_time  total_budget  acq_budget  lib_visitors   loans  \\\\\\n0       0.00000       0.00000     0.00000       0.00000 0.00000   \\n1       0.00000       0.19895     0.16693       0.08073 0.12341   \\n2       0.00000       0.01445     0.10711       0.00692 0.02567   \\n3       0.00847       0.03151     0.18009       0.01516 0.01830   \\n4       0.08475       0.19895     0.16693       0.11382 0.05283   \\n...         ...           ...         ...           ...     ...   \\n8179    0.05085       0.03138     0.03883       0.02526 0.01344   \\n8180    0.00000       0.06657     0.39589       0.02020 0.01080   \\n8181    0.00847       0.01535     0.02627       0.00998 0.01166   \\n8182    0.01695       0.00000     0.00000       0.00027 0.00070   \\n8183    0.00847       0.02020     0.01354       0.00881 0.00960   \\n\\n      user_ed_sessions  vuln_group_budget  total_settlement  \\\\\\n0              0.00000            0.00000           0.01151   \\n1              0.00022            0.00000           0.04425   \\n2              0.00000            0.00000           0.00519   \\n3              0.00001            0.00000           0.01436   \\n4              0.00022            0.00000           0.04425   \\n...                ...                ...               ...   \\n8179           0.00011            0.00043           0.00591   \\n8180           0.00020            0.00001           0.00427   \\n8181           0.00044            0.00000           0.00294   \\n8182           0.00000            0.00000           0.00000   \\n8183           0.00165            0.00000           0.00391   \\n\\n      acq_budget_settlement  \\n0                   0.02815  \\n1                   0.23447  \\n2                   0.01188  \\n3                   0.20793  \\n4                   0.23447  \\n...                     ...  \\n8179                0.05916  \\n8180                0.01520  \\n8181                0.02523  \\n8182                0.00000  \\n8183                0.01329  \\n\\n[8184 rows x 20 columns]']\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 모델과 데이터 준비 (예: 모델과 X_train)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m explainer \u001b[38;5;241m=\u001b[39m \u001b[43mshap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeepExplainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 모델과 데이터\u001b[39;00m\n\u001b[1;32m      6\u001b[0m shap_values \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mshap_values(X_train)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Summary plot\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/jw_tensorflow/lib/python3.11/site-packages/shap/explainers/_deep/__init__.py:90\u001b[0m, in \u001b[0;36mDeepExplainer.__init__\u001b[0;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(model, masker)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m \u001b[43mTFDeep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_phase_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m framework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpytorch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexplainer \u001b[38;5;241m=\u001b[39m PyTorchDeep(model, data)\n",
      "File \u001b[0;32m~/anaconda3/envs/jw_tensorflow/lib/python3.11/site-packages/shap/explainers/_deep/deep_tf.py:161\u001b[0m, in \u001b[0;36mTFDeep.__init__\u001b[0;34m(self, model, data, session, learning_phase_flags)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m#if type(self.model)is tuple:\u001b[39;00m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;66;03m#    self.fModel(cnn.inputs, cnn.get_layer(theNameYouWant).outputs)\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpected_value \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_between_tensors(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_output\u001b[38;5;241m.\u001b[39mop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_inputs)\n",
      "File \u001b[0;32m~/anaconda3/envs/jw_tensorflow/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/jw_tensorflow/lib/python3.11/site-packages/keras/src/engine/input_spec.py:235\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m shape\u001b[38;5;241m.\u001b[39mrank\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;241m!=\u001b[39m spec\u001b[38;5;241m.\u001b[39mndim:\n\u001b[0;32m--> 235\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    236\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    237\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    238\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    239\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m         )\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmax_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;241m.\u001b[39mrank\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'sequential' (type Sequential).\n\nInput 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (8184, 20)\n\nCall arguments received by layer 'sequential' (type Sequential):\n  • inputs=['      dom_books  for_books  nonbook_mat  elec_mat  serials  ann_growth  \\\\\\n0       0.00000    0.00000      0.00000   0.00000  0.00431     0.00033   \\n1       0.08417    0.00000      0.01129   0.00026  0.00280     0.00142   \\n2       0.04455    0.00000      0.00452   0.00000  0.00374     0.00670   \\n3       0.33166    0.00000      0.01953   0.00107  0.01013     0.01156   \\n4       0.11052    0.00000      0.02472   0.00026  0.00676     0.00309   \\n...         ...        ...          ...       ...      ...         ...   \\n8179    0.05988    0.03277      0.00641   0.00039  0.00359     0.00306   \\n8180    0.04843    0.00229      0.00786   0.00049  0.00036     0.00171   \\n8181    0.02278    0.00318      0.00471   0.00255  0.00122     0.00192   \\n8182    0.00745    0.01823      0.00000   0.00000  0.02817     0.00066   \\n8183    0.04277    0.01639      0.00998   0.00000  0.00539     0.00090   \\n\\n      lib_site_area  lib_build_area  total_seats  user_comps  full_time  \\\\\\n0           0.00610         0.00725      0.09126     0.00000    0.00000   \\n1           0.00516         0.01455      0.19263     0.09023    0.15714   \\n2           0.00284         0.00496      0.04816     0.06391    0.07143   \\n3           0.00291         0.00314      0.04816     0.05639    0.12857   \\n4           0.00157         0.01015      0.19263     0.11278    0.21429   \\n...             ...             ...          ...         ...        ...   \\n8179        0.00507         0.01481      0.05851     0.08647    0.07143   \\n8180        0.00227         0.00531      0.05177     0.12030    0.10000   \\n8181        0.00027         0.00115      0.02071     0.00376    0.05714   \\n8182        0.00154         0.00062      0.01156     0.00000    0.05714   \\n8183        0.00195         0.00156      0.03299     0.00752    0.07143   \\n\\n      part_time  total_budget  acq_budget  lib_visitors   loans  \\\\\\n0       0.00000       0.00000     0.00000       0.00000 0.00000   \\n1       0.00000       0.19895     0.16693       0.08073 0.12341   \\n2       0.00000       0.01445     0.10711       0.00692 0.02567   \\n3       0.00847       0.03151     0.18009       0.01516 0.01830   \\n4       0.08475       0.19895     0.16693       0.11382 0.05283   \\n...         ...           ...         ...           ...     ...   \\n8179    0.05085       0.03138     0.03883       0.02526 0.01344   \\n8180    0.00000       0.06657     0.39589       0.02020 0.01080   \\n8181    0.00847       0.01535     0.02627       0.00998 0.01166   \\n8182    0.01695       0.00000     0.00000       0.00027 0.00070   \\n8183    0.00847       0.02020     0.01354       0.00881 0.00960   \\n\\n      user_ed_sessions  vuln_group_budget  total_settlement  \\\\\\n0              0.00000            0.00000           0.01151   \\n1              0.00022            0.00000           0.04425   \\n2              0.00000            0.00000           0.00519   \\n3              0.00001            0.00000           0.01436   \\n4              0.00022            0.00000           0.04425   \\n...                ...                ...               ...   \\n8179           0.00011            0.00043           0.00591   \\n8180           0.00020            0.00001           0.00427   \\n8181           0.00044            0.00000           0.00294   \\n8182           0.00000            0.00000           0.00000   \\n8183           0.00165            0.00000           0.00391   \\n\\n      acq_budget_settlement  \\n0                   0.02815  \\n1                   0.23447  \\n2                   0.01188  \\n3                   0.20793  \\n4                   0.23447  \\n...                     ...  \\n8179                0.05916  \\n8180                0.01520  \\n8181                0.02523  \\n8182                0.00000  \\n8183                0.01329  \\n\\n[8184 rows x 20 columns]']\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 모델과 데이터 준비 (예: 모델과 X_train)\n",
    "explainer = shap.DeepExplainer(model, X_train)  # 모델과 데이터\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 0  \n",
    "\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0][sample_index], X_train.iloc[sample_index], matplotlib=True, figsize=(42, 5))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jw_tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
